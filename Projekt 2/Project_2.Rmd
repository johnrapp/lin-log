---
title: "Project 2"
author: "Axel Sjöberg (ax3817sj-s) & John Rapp Farnes (jo5113fa-s)"
date: "16 maj 2019"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
classoption: a4paper
---

```{r setup_knitr, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  collapse = TRUE,
  comment = "#>"
)

options(tinytex.verbose = TRUE)
```

```{r setup_pdf, eval = FALSE}
# Install from CRAN
install.packages('rmarkdown')
# Render PDF
install.packages("tinytex")
tinytex::install_tinytex()  # install TinyTeX

```


```{r init_vars, include = FALSE}
library(ggplot2)
library(knitr)
library(dplyr)
library(purrr)
library(pscl)
library(GGally)

cdi <- read.delim("CDI.txt")
 
region.names <- c("Northeast", "Midwest", "South", "West")
cdi$region <- factor(cdi$region, levels = c(1, 2, 3, 4),
                     labels = region.names)
  
cdi$crm1000 <- 1000 * cdi$crimes / cdi$popul
cdi$phys1000 = 1000 * cdi$phys / cdi$popul


median = summary(cdi$crm1000)["Median"]

cdi <- cbind(cdi, hicrm = ifelse(cdi$crm1000 < median, 0, 1))
```


\newpage

# Introduction

## Background and dataset
The objective of this report is to determine which covarites that can be used to predict if a US county has a low or high crime rate (serious crimes per 1000 inhabitants). The dataset used to do this is county demographic information (CDI) for 440 of the most populous counites in the US 1990-1992. Each county records includes data on the 14 variables listed below in table \ref{tab:cdi}. Counties with missing data has been removed from the dataset.


```{r}
variables = c("id", "county", "state", "area", "popul", "pop1834", "pop65plus", "phys", "beds", "crimes", "higrads", "bachelors", "poors", "unemployed", "percapitaincome", "totalincome", "region")

descriptions = c("identification number, 1–440", "county name", "state abbreviation", "land area (square miles)", "estimated 1990 population", "percent of 1990 CDI population aged 18–34", "percent of 1990 CDI population aged 65 years old or older", "number of professionally active nonfederal physicians during 1990", "total number of beds, cribs and bassinets during 1990", "total number of serious crimes in 1990", "percent of adults (25 yrs old or older) who completed at least 12 years of school", "percent of adults (25 yrs old or older) with bachelor’s degree", "Percent of 1990 CDI population with income below poverty level", "percent of 1990 CDI labor force which is unemployed", "per capita income of 1990 CDI population (dollars)", "total personal income of 1990 CDI population (in millions of dollars)", "Geographic region classification used by the U.S. Bureau of the Census,
                   including Northeast, Midwest, South and West")

variable.table = cbind(variables, descriptions)

kable(variable.table, caption = "\\label{tab:cdi}CDI dataset columns", col.names = c("Variable", "Description"))
```

In order to measure crime rate, another varible called `crm1000` was added, descibing the number of serious crimes per 1000 inhabitants. Using `crm1000`, counties were divided into counties with high or non-high crime rate, where counties with crime rate higher than the median of `crm1000` in the dataset were categorized as having a high crime rate. This crime status of the county was stored in another column called `hicrm`, which takes the value 1 if the county was a high crime county and 0 if it was a low crime county. In this paper, `hicrm` is used as the dependent varible. Similar to crime rate, a variable `phys1000` was also added, measuring the number of physicians per 1000 inhabitants.

## Model
The binary dependent variable was modelled using a logistic regression model. This model assumes that the log-odds of a certain observation $i$ is a linear combination of its covariates $X_{j,i}$ and parameters $\beta_i$, as well as errors $\epsilon_i$. As such, the model looks like:
\begin{equation}
  \ln{\frac{p_i}{1 - p_i}} = \beta_0 + \sum_j\beta_{j} \cdot X_{j,i} + \epsilon_i
\end{equation}
In contrast to linear regression, the error terms $\epsilon_i$ are not assumed to have any particular distribution.

# Analysis

## The higrad model

### Introduction

The first model considered has `higrads` as the sole covariate. As such, the model becomes:

\begin{equation}
  \ln{\frac{p_i}{1 - p_i}} = \beta_0 + \beta_{higrads} \cdot X_{higrads,i} + \epsilon_i
\end{equation}


In order to determine if there is a relationship between `hicrm` and `higrads` they were plotted against each other. Because `hicrm` is a binary varible it was very difficult to determine if a relationship exists only by the looking at the pattern in the plot. In order to clarify this relationship, a kernel smoother was added to the plot, where a smooth line was attained with a bandwidth of 20. In addition, the fitted model along with its 95 % confidence interval were included. Figure 1 shows the attained plot described above.


```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 5, fig.cap="\\label{fig:hicrm_higrads}Plot of \\texttt{hicrm} against \\texttt{higrads}, including kernel smoothing and prediction of fitted model with 95 \\% confidence interval"}
with(cdi, {
   plot(hicrm ~ higrads)
   lines(ksmooth(higrads, hicrm, bandwidth = 20))
})

model.higrads <- glm(hicrm ~ higrads, data = cdi, family = "binomial")
sum <- summary(model.higrads)

x0 <- data.frame(higrads = seq(0, 100, 1))

predx <- cbind(x0, prob = predict(model.higrads, x0, type = "response"))
with(predx, lines(higrads, prob, col = "blue"))

# calculate conf.int for the linear part x*beta:
standard.error <- 1.96
xb <- predict(model.higrads, x0, se.fit = TRUE)
ci.xb <- data.frame(lwr = xb$fit - standard.error * xb$se.fit,
                    upr = xb$fit + standard.error * xb$se.fit)

# transform to CI for the odds:
ci.odds <- exp(ci.xb)

to.prob = function(odds) {
  return (odds / (1 + odds))
}

# and finally CI for the probabilities and add to the plot:
predx <- cbind(predx, to.prob(ci.odds))
with(predx, {
  lines(higrads, lwr, lty = 2, col = "red")
  lines(higrads, upr, lty = 2, col = "red")
})

```
As seen in figure \ref{fig:hicrm_higrads}, the kernel curve looks S-shaped, implying that a logistic model may be appropriate to describe the relationship. Further, the S-shape is "downward facing", implying a negative $\beta_{higrads}$, meaning that the ∂ability of a county being classified as a high crime decreases when the amount of higrads in the county increases. Another thing to note in figure \ref{fig:hicrm_higrads} is how few data points exists with `higrads` below 60, meaning that significance is low in this region. In addition, looking at points with `higrads` over 60, the kernel curve and fitted line only cover about 30% - 70%, implying that `higrads` alone may not predict `hicrm` well.


### Fitted model and significance

In order to study the significance of the model, the $\beta$ values together with their 95 % confidence inteval are presented in table \ref{tab:higrad_beta}.



```{r beta_confidence}
coeff.beta <- model.higrads$coefficients

ci.beta <- suppressMessages(confint(model.higrads))

beta.p <- sum$coefficients[, "Pr(>|z|)"]

table.beta.ci <- cbind("Estimate" = coeff.beta, ci.beta, "P-value" = beta.p)
row.names(table.beta.ci) <- c("$\\beta_0$", "$\\beta_{higrads}$")

table.caption <- "\\label{tab:higrad_beta}$\\beta$-values of \\texttt{higrad} model, with 95 \\% confidence inteval"
kable(table.beta.ci, caption = table.caption, digits = c(rep(2,3), 5))

```

```{r beta_decrease, include = F}
or.higrads <- exp(coeff.beta["higrads"])

decrease.one.percent <- 1 - or.higrads
decrease.ten.percent <- 1 - or.higrads^10

display.percent <- function(value) {
  return(round(value * 100, digits=1))
}

decrease.one.percent
decrease.ten.percent
```


As seen in the table 2, all of the P-values are < 0.05, meaning both that conties with no `higrads` has greater than 0 probability of having a high crime rate, and that `higrads` has a statistically significant effect on `hicrm`. This effect can be measured by looking at $e^{\beta_{higrads}}$, showing that an increase of 1% in `higrads` decreases odds of `hicrm` by `r display.percent(decrease.one.percent)`%, while an increase of 10% decreases odds of by `r display.percent(decrease.ten.percent)`%.

### Model predictions


Using the higrads model: the probability, with a 95 % confidence interval, of having a high crime rate in a county where the amount of higrads is 65 (percent), and where it is 85 (percent) were predicted. The result is presented in table \ref{tab:higrads_predictions}.

```{r predict}
x0 <- data.frame(higrads = c(65, 85))


predx <- cbind(x0, prob = predict(model.higrads, x0, type = "response"))

xb <- predict(model.higrads, x0, se.fit = TRUE)
ci.xb <- data.frame(lwr = xb$fit - standard.error * xb$se.fit,
                    upr = xb$fit + standard.error * xb$se.fit)
# transform to CI for the odds:
ci.odds <- exp(ci.xb)

# and finally CI for the probabilities and add to the plot:
predx <- cbind(predx$higrads, predx$prob * 100, to.prob(ci.odds) * 100)

table.caption <- "\\label{tab:higrads_predictions}Predictions of `higrads` model"
kable(predx, caption = table.caption, col.names = c("Higrads", "Probability (%)", "2.5 %", "97.5 %"), digits = 1)
```

Looking at table \ref{tab:higrads_predictions}, one can see that a county with 65 % high education graduates has greater than 50% probability of having a high crime rate, while a county with 85 % high education graduates has less than 50% probability of having a high crime rate. Further, these results are also statistically significant.

### Model performance analysis

```{r pred_sense_spec, include = F}

calc.sense.spec = function(model) {
  pred.counties <- cdi

  pred.counties <- cbind(pred.counties, prob.hicrm = predict(model, pred.counties, type = "response"))
  pred.counties <- cbind(pred.counties, pred.hicrm = ifelse(pred.counties$prob.hicrm > 0.5, 1, 0))
  
  num.hicrm <- sum(pred.counties$hicrm)
  
  # Sensitivity
  true.postive <- with(pred.counties, ifelse(pred.hicrm == 1 & hicrm == 1, 1, 0))
  sensitivity <- sum(true.postive) / num.hicrm
  sensitivity
  
  # Specificity
  true.negative <- with(pred.counties, ifelse(pred.hicrm == 0 & hicrm == 0, 1, 0))
  specificity <- sum(true.negative) / num.hicrm
  specificity
  
  metrics <- data.frame(
    sensitivity,
    specificity
  )
  
  return(metrics)
}

sense.spec.higrads <- calc.sense.spec(model.higrads)

sensitivity.higrads <- sense.spec.higrads$sensitivity
specificity.higrads <- sense.spec.higrads$specificity

```

In order to analyze model performance, the sensitivity and specificity of the model were calculated. The sensitivity of a model is the ratio of predicted positives to real positives in the dataset, while the specificity of a model is the ratio of predicted negatives to real negatives in the dataset. As such, the higher the value of the sensitivity and specificity, the better.

The sensitivity of the model was `r display.percent(sensitivity.higrads)`% and the specificity of the model was `r display.percent(specificity.higrads)`%, indicating that the model does not clasify the crime rate of the counties rather successfully.

## The region model

### Introduction

Next, a logistic model was adopted based on the `region` covariate. Since `region` is not continous, but categorial, it was modelled using "dummy variables" $X_i$. In order to implement this effectively, one of the categories is chosen as a reference category, and the effects of the other categories are measured in comparison to it.

In order to determine which region to use as reference category, a cross-tabulation of the data between `region` and `hirm` had to be studied, see table \ref{tab:cross-tabulation}.

\newpage

```{r cross_tabulation}

cross.table <- table(cdi %>% select(region, hicrm))

table.caption <- "\\label{tab:cross-tabulation}Cross-tabulation between \\texttt{region} and \\texttt{hicrm}"
kable(cross.table, col.names = c("Low crime", "High crime"), caption = table.caption)

reference.level <- "South"
cdi$region <- relevel(cdi$region, ref = reference.level)

```





As a reference region, the one that has the largest number of counties in it’s smallest low/high category was chosen. As a tie-breaker, the other low/high category was used. This approach produces the lowest standard error, and therefore highest significance. As seen in table \ref{tab:cross-tabulation}, the above given condition resulted in choosing `r reference.level` as the reference region.


Using this reference region, the logistic model became
\begin{equation}
  \ln{\frac{p_i}{1 - p_i}} = \beta_0 + \beta_{Northeast} \cdot X_{Northeast,i} + \beta_{Midwest} \cdot X_{Midwest,i} + \beta_{West} \cdot X_{West,i} + \epsilon_i
\end{equation}

Here, the $\beta$ coefficients are measured relative to `r reference.level`, while $\beta_0$ was the log-odds coefficient for `r reference.level`.

### Fitted model and significance

The model was fit with the given data set, estimating $\beta_i$, shown together with its 95 % confidence interval and P-value in table \ref{tab:region_beta}.

```{r region_beta}
model.region <- glm(hicrm ~ region, data = cdi, family = "binomial")
sum <- summary(model.region)

coeff.beta <- model.region$coefficients

ci.beta <- suppressMessages(confint(model.region))

beta.p <- sum$coefficients[, "Pr(>|z|)"]

table.beta.ci <- cbind("Estimate" = coeff.beta, ci.beta, "P-value" = beta.p)
row.names(table.beta.ci) <- c("$\\beta_0$", "$\\beta_{Northeast}$", "$\\beta_{Midwest}$", "$\\beta_{West}$")

table.caption <- "\\label{tab:region_beta}$\\beta$-estimates for the \\texttt{region} model, together with 95 \\% confidence interval and P-values"
kable(table.beta.ci, caption = table.caption, digits = c(rep(2,3), 3))
```

As may be seen in table \ref{tab:region_beta}, the P-value for $\beta_{West}$ was > 0.05, indicating a lack of statistical significance in the difference between how South and West predicts `hicrm`.

Next, the odds-ratios for the different categories were determined. The odds-ratios measure the odds of a particular category in relation to the reference category. These may be calculated as $OR_i = e^{\beta_i}$ and are presented in table \ref{tab:region_OR}.


```{r}
beta.region <- exp(coeff.beta)

table.beta.ci <- cbind("OR" = exp(coeff.beta), exp(ci.beta))
table.beta.ci <- table.beta.ci[-c(1), ]

row.names(table.beta.ci) <- region.names[region.names != reference.level]

table.caption <- "\\label{tab:region_OR}Odds-ratios for the \\texttt{region} model, together with 95 \\% confidence interval"
kable(table.beta.ci, caption = table.caption, digits = 2)


```

As seen in table \ref{tab:region_OR}, the odds-ratios were less than 1 for all categories but the reference region. This implies that the odds for all regions are lower compared to the reference region, i.e. that the probability of a high crime rate is lower in all regions compared to the reference region. This is however not statistically significant as the confidence interval for the western region's odds ratio, in relation to the refernce category, cover 1. This coincides with the findings in table  \ref{tab:region_beta} where the $\beta_{West}$ P-value was > 0.05. 



### Model predictions

Using the fitted model, the probabilies of having a high crime rate, with confidence interval, for the different regions was determined, shown in table \ref{tab:region_prob}.


```{r}


pred.counties <- subset(cdi, select = c(county, region, hicrm))

x0 <- data.frame(region = region.names)

pred <- predict(model.region, x0, se.fit = TRUE, type = "response")


## confidence interval for log-odds
ci.prob <- cbind("2.5 %" = pred$fit - standard.error * pred$se.fit, 
                "97.5 %" = pred$fit + standard.error * pred$se.fit)


table.prob.ci <- cbind("Probability (%)" = pred$fit * 100, ci.prob * 100)
row.names(table.prob.ci) <- region.names

table.caption <- "\\label{tab:region_prob}Probability of high crime rate (\\%), together with 95 \\% confidence interval for each of the regions"

kable(table.prob.ci, caption = table.caption, digits = 1)


```

### Model performance analysis


```{r, include = F}
sense.spec.region <- calc.sense.spec(model.region)

sensitivity.region <- sense.spec.region$sensitivity
specificity.region <- sense.spec.region$specificity

```

For the `region` model, the sensitivity was `r display.percent(sensitivity.region)`%, while the specificity was `r display.percent(specificity.region)`%. Comparing the two models analyzed so far, the `region` model performs better measured on sensitivity and specificity, as seen in table \ref{tab:compare_sense_spec_region_higrad}

```{r}

compare.sense <- data.frame(
  "Covariate" = c("Higrads", "Region"),
  "Sensitivity" = c(sensitivity.higrads, sensitivity.region) * 100,
  "Specificity" = c(specificity.higrads, specificity.region) * 100
);

table.caption <- "\\label{tab:compare_sense_spec_region_higrad}Comparison of sensitivity and specificity of \\texttt{higrad} and \\texttt{region} model"
kable(compare.sense, caption = table.caption, col.names = c("Covariate", "Sensitivity (%)", "Specificity (%)"), digits = 1)

```

## Combined model and comparison

### Introduction
Next a model that used both `higrads` and `region` was analyzed, i.e. the following model:

\begin{equation}
  \ln{\frac{p_i}{1 - p_i}} = \beta_0 + \beta_{Northeast} \cdot X_{Northeast,i} + \beta_{Midwest} \cdot X_{Midwest,i} + \beta_{West} \cdot X_{West,i} + \beta_{higrads} \cdot X_{higrads,i} + \epsilon_i
\end{equation}

### Model comparison

In order to compare the models, metrics other than sensitivity and specificity was studied. These were AIC and BIC, which are penalized-likelihood criteria and Nagelkerke psuedo $R^2$, which is a measurment that increses up to 1 the better the model fit is. As they are defined, AIC and BIC should be as low as possible for a model to be performant, while psuedo $R^2$ should be as high as possible.

A comparison of the models with regards to AIC, BIC and Psuedo $R^2$ can be seen in figure \ref{fig:comparison_region_higrads_both}, while a comparison of sensitivity and specificity is found in table \ref{tab:compare_sense_spec_region_higrad_both}.

```{r, include = F}
model.both <- glm(hicrm ~ higrads + region, data = cdi, family = "binomial")

model.names <- c("Higrads", "Region", "Both")
models <- list(model.higrads, model.region, model.both)

calc.metrics = function(model) {
  calc.aic <- AIC(model)
  calc.bic <- AIC(model, k = log(nrow(cdi)))
  sense.spec <- calc.sense.spec(model)
  
  pseudo.R2 <- pR2(model)["r2CU"]
  
  metrics <- data.frame(
    aic = calc.aic,
    bic = calc.bic,
    sensitivity = sense.spec$sensitivity,
    specificity = sense.spec$specificity,
    PseudoR2 = pseudo.R2
  )
  
  return (metrics)
}
calc.metrics.models = function(models, model.names) {
  models.metrics <- lapply(models, calc.metrics)

  compare.models <- do.call(rbind, models.metrics)
  compare.models <- cbind(model.name = model.names, compare.models)
  rownames(compare.models) <- NULL
  
  return (compare.models)  
}

compare.models <- calc.metrics.models(models, model.names)
compare.models
```



```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 5, fig.cap="\\label{fig:comparison_region_higrads_both}Comparison of AIC and BIC and Nagelkerke psuedo $R^2$ for the different models"}

n <- nrow(compare.models)
x <- 1:n
with(compare.models, {
  par(mar = c(5, 4, 4, 4) + 0.3)  # Leave space for second axis
  
  plot(x, rep(0, n), type = "n", ylim = c(500, 650), xaxt = "n",
       ylab = "AIC/BIC", xlab = "Model",
       main = "Model comparison")
  axis(1, at = x, labels = model.name)
  lines(x, aic, col = "red", lty = 2, lwd = 2)
  lines(x, bic, col = "red", lty = 1, lwd = 2)
  
  par(new = TRUE)
  plot(x, rep(0, n), type = "n", axes = FALSE, ylim = c(0, 1), xaxt = "n",
     xlab = "", ylab = "")
  lines(x, PseudoR2, col = "blue", lty = 1, lwd = 2)
  best.model <- 3
  axis(side=4, at = pretty(c(0, 1)))
  mtext("Pseudo R^2", side=4, line=3)
  
  legend("topright",legend=c("BIC", "AIC","Pseudo R^2"),
    text.col=c("red","red", "blue"), lty=c(1, 2, 1),col=c("red","red", "blue"))
  
})
```

```{r}
compare.sense <- compare.models %>% select(model.name, sensitivity, specificity)
compare.sense$sensitivity <- compare.models$sensitivity * 100
compare.sense$specificity <- compare.models$specificity * 100

table.caption <- "\\label{tab:compare_sense_spec_region_higrad_both}Comparison of sensitivity and specificity of models"
kable(compare.sense, caption = table.caption, col.names = c("Covariate", "Sensitivity (%)", "Specificity (%)"), digits = 1)

```

As seen in figure \ref{fig:comparison_region_higrads_both} and table \ref{tab:compare_sense_spec_region_higrad_both}, the combined model with both the covariates performed the best on all the studied metrics.

\newpage

### Combined model analysis

The combined model was further analyzed by studying the QQ-plot (see figure \ref{fig:qq-plot_combined}), the squared standardized Pearson residuals and the standardized deviance residuals against the linear predictor $X\beta$ (see figure \ref{fig:residuals_combined}). Furthermore the Cook’s distance was plotted against the linear predictor, `higrads` and against `region` was plotted (see figure \ref{fig:cooks_combined}).


```{r, out.extra = '', fig.pos = "h", fig.width = 6, fig.height = 4, fig.cap="\\label{fig:qq-plot_combined}QQ-plot for the combined model"}
model.plot <- model.both
influence.plot <- influence(model.plot)
  
xb <- predict(model.plot)

r <- influence.plot$pear.res / sqrt(1 - influence.plot$hat)
ds <- influence.plot$dev.res / sqrt(1 - influence.plot$hat)

qqnorm(r)
qqline(r)

```

\newpage

From the QQ-plot it looks like the model residuals follow a bimodal distribution rather than a normal distribution. 

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:residuals_combined}Standardized Pearson residuals as well as standardized deviance residuals for the combined model, against the linear predictor $X\\beta$"}
par(mfrow = c(3, 1))

with(cdi, plot(r ~ xb, ylab = "Squared Pearson residuals", main = "Standardized Pearson residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

with(cdi, plot(r^2 ~ xb, ylab = "Squared Pearson residuals", main = "Squared standardized Pearson residuals against linear predictor"))
abline(h = c(0, 4), col = "red", lty = 3)

with(cdi, plot(ds ~ xb, ylab = "Deviance residuals", main = "Standardized deviance residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

```


For a standardized pearson residual to be considered suspiciously large, $|r_i| > |\lambda_{\alpha/2}| \approx 2$. This was true for 8 of the standadized pearson residuals, which can be seen in figure \ref{fig:residuals_combined}, where the standardized pearson residuals were plotted against the linear predictor. For a deviance residual to be considered too large $|r_i| > 2$. This was never the case which is verified by figure \ref{fig:residuals_combined} where the standardized devience residuals were plotted against the linear predictor. 

In order to measure how the $\beta$-estimates were influenced by indiviual observations, Cook's distance for logistic regression was calcultaed and plotted, see figure \ref{fig:cooks_combined}

\newpage


```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:cooks_combined}Cook's distance, for the combined model, against linear predictor, region as well as higrads"}
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))

n <- nrow(cdi)
cook.plot <- cooks.distance(model.plot)

with(cdi, plot(cook.plot ~ xb, ylab = "Cook's distance", main = "Cook's distance against linear predictor"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ region, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ higrads, ylab = "Cook's distance", main = "Cook's distance against higrads"))
abline(h = c(1, 4 / n), col = "red", lty = 2)


```

As can be seen in figure \ref{fig:cooks_combined} the vast majoriy of the the observations are below the horizontal line. The amount of counties with a high Cook's distance are limited to 3-4. The data entry with the largest Cook's distance is found in the South region while the rest of the counties with a high Cook's distance are found in the West region. The Cook's distance plots give no concern to why the combined model should be amiss.


\newpage

## Interaction model
### Introduction
As a forth model, interaction terms were also considered, building in that the effect of higrads may be different in different regions, where the log-odds in the model includes interaction terms such as $\beta_{Northeast * higrads} \cdot X_{higrads,i}$.


### Model analysis

```{r, include = F}
model.interaction <- glm(hicrm ~ higrads * region, data = cdi, family = "binomial")

anova.combined.interaction <- anova(model.interaction, model.both, test = "LRT")
anova.combined.interaction
likelihood.p_value <- anova.combined.interaction$`Pr(>Chi)`[2]

```


The performance of the interaction model compared to the combined model was analyzed by the likelihood test. This test is similar to a partial F-test, but adapted for logistical regression, using likelihoods since sums of squares are not applicable. The likelihood test resulted in the interaction model being significantly better than the combined model, with a P-value of `r round(likelihood.p_value, 3)`.

In addition, the AIC, BIC, Nagelkerke, sensitivity and specificity was compared to the combined model, in table \ref{tab:compare_sense_spec_interaction}. The interaction model was analyzed by studying a QQ-plot (see figure \ref{fig:qq-plot_interaction}) the squared standardized Pearson residuals and the standardized deviance residuals against the linear predictor $X^\beta$ (see figure \ref{fig:residuals_interaction}). Furthermore the Cook’s distance was plotted against the linear predictor, `higrads` and against `region` (see figure \ref{fig:cooks_interaction}).


```{r}

models <- list(model.both, model.interaction)
model.names <- c("Combined model", "Interaction model")

compare.models <- calc.metrics.models(models, model.names)

compare.models$sensitivity <- compare.models$sensitivity * 100
compare.models$specificity <- compare.models$specificity * 100

col.names <- c("Covariate", "AIC", "BIC", "Sensitivity (%)", "Specificity (%)", "Pseudo R2")
table.caption <- "\\label{tab:compare_sense_spec_interaction}Comparison of sensitivity and specificity of models"
kable(compare.models, caption = table.caption, col.names = col.names, digits = c(rep(0, 5), 2))

```


```{r, out.extra = '', fig.pos = "h", fig.width = 6, fig.height = 4, fig.cap="\\label{fig:qq-plot_interaction}QQ-plot for the integration model"}
model.plot <- model.interaction
influence.plot <- influence(model.plot)
  
xb <- predict(model.plot)

r <- influence.plot$pear.res / sqrt(1 - influence.plot$hat)
ds <- influence.plot$dev.res / sqrt(1 - influence.plot$hat)

qqnorm(r)
qqline(r)

```


\newpage


```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:residuals_interaction}Standardized Pearson residuals as well as standardized deviance residuals for the interaction model, against the linear predictor $X\\beta$"}
par(mfrow = c(3, 1))

with(cdi, plot(r ~ xb, ylab = "Squared Pearson residuals", main = "Standardized Pearson residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

with(cdi, plot(r^2 ~ xb, ylab = "Squared Pearson residuals", main = "Squared standardized Pearson residuals against linear predictor"))
abline(h = c(0, 4), col = "red", lty = 3)

with(cdi, plot(ds ~ xb, ylab = "Deviance residuals", main = "Standardized deviance residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)
```

The interaction had more Person residuals that may be considered suspisiously large than the combined model. Unlike the combined model it also had standardized devience residuals that are too large. 


\newpage


```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:cooks_interaction}Cook's distance, for the interaction model, against linear predictor, region as well as higrads"}
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))

n <- nrow(cdi)
cook.plot <- cooks.distance(model.plot)

with(cdi, plot(cook.plot ~ xb, ylab = "Cook's distance", main = "Cook's distance against linear predictor"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ region, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ higrads, ylab = "Cook's distance", main = "Cook's distance against higrads"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

```
Both the interaction and the combined model performed better on some metrics, while performing worse on other. The interaction model had a lower AIC value but at the same time higher BIC-value. This is expected since BIC penalizes larger models more than the AIC value tends to do. The sensitivity, specificity and Pseudo $R^2$ values were worse for the Combined model. The interaction model performed considerably worse than the combined model on the Cook's distance acount. The interaction model had far more outliers and outliers with a much higher Cook's distance. 

As aforementioned, the interaction model had some credibility issues with its residuals. Moreover the interaction model had far more outliers as well as considerably larger Cook's distance than the combined model. At the same time the interaction model outperformed the combined model on likelyhood ratio test, AIC, Sensitivity, Specificity and Pseudo R2. The outliers in the interaction model might therefor not effect the Interaction model to much. In the end, the interaction model is favoured over the combined model.

## Finding the optimal model

### Methology

Next, an attempt to fit an optimal model to predict high crime rates was made, using the previous covariates, as well as `poors` and `pshys1000`. When searching for an optimal model, interaction terms were disregarded.

Models of increasing complexity were constructed by adding more covariates. These were then compared to each other on the used metrics, i.e. AIC, BIC, Pseudo $R^2$, sensitivity and specificity. In addition, the result of automatic selection using R `step` function was studied.

### Model comparison

AIC, BIC and Pseudo $R^2$ of the studied model are shown in figure \ref{fig:comparison_optimal}. In addition, table \ref{tab:comparison_optimal} includes sensitivity and specificity for the different models. 
```{R, include = F}
model.1 <- glm(hicrm ~ higrads, data = cdi, family = "binomial")
model.2 <- glm(hicrm ~ higrads + region, data = cdi, family = "binomial")
model.3 <- glm(hicrm ~ higrads + region + poors, data = cdi, family = "binomial")
model.4 <- glm(hicrm ~ higrads + region + poors + phys1000, data = cdi, family = "binomial")

model.5 <- glm(hicrm ~ region + poors + phys1000, data = cdi, family = "binomial")
model.6 <- glm(hicrm ~ higrads + region + phys1000, data = cdi, family = "binomial")


model.names <- c("H", "H + R", "H + R + Po", "H + R + Po + Phy", "R + Po + Phy", "H + R + Phy")
models <- list(model.1, model.2, model.3, model.4, model.5, model.6)

compare.models <- calc.metrics.models(models, model.names)

compare.models

step(model.4, k = log(nrow(cdi)))

anova(model.4, model.5, test = "LRT")
```

```{r, out.extra = '', fig.pos = "h", fig.width = 10, fig.height = 5, fig.cap="\\label{fig:comparison_optimal}Comparison of AIC and BIC and Nagelkerke psuedo $R^2$ for the different models. Key: H = \\texttt{higrads}, R = \\texttt{region}, Po = \\texttt{poors}, Phy = \\texttt{phys1000}"}

n <- nrow(compare.models)
x <- 1:n
with(compare.models, {
  par(mar = c(5, 4, 4, 4) + 0.3)  # Leave space for second axis
  
  plot(x, rep(0, n), type = "n", ylim = c(450, 650), xaxt = "n",
       ylab = "AIC/BIC", xlab = "Model",
       main = "Model comparison")
  axis(1, at = x, labels = model.name)
  lines(x, aic, col = "red", lty = 2, lwd = 2)
  lines(x, bic, col = "red", lty = 1, lwd = 2)
  
  par(new = TRUE)
  plot(x, rep(0, n), type = "n", axes = FALSE, ylim = c(0, 1), xaxt = "n",
     xlab = "", ylab = "")
  lines(x, PseudoR2, col = "blue", lty = 1, lwd = 2)
  best.model <- 3
  axis(side=4, at = pretty(c(0, 1)))
  mtext("Pseudo R^2", side=4, line=3)
  
  legend("topright",legend=c("BIC", "AIC","Pseudo R^2"),
    text.col=c("red","red", "blue"), lty=c(1, 2, 1), col=c("red","red", "blue"))
})

```

```{r}

compare.models$sensitivity <- compare.models$sensitivity * 100
compare.models$specificity <- compare.models$specificity * 100

col.names <- c("Model", "AIC", "BIC", "Sensitivity (%)", "Specificity (%)", "Pseudo R2")
table.caption <- "\\label{tab:comparison_optimal}Comparison of sensitivity and specificity of models. Key: H = \\texttt{higrads}, R = \\texttt{region}, Po = \\texttt{poors}, Phy = \\texttt{phys1000}"
kable(compare.models, caption = table.caption, col.names = col.names, digits = c(rep(0, 5), 2))

```

The results in \ref{fig:comparison_optimal} and \ref{tab:comparison_optimal} show that the `region` + `poors` + `phys1000` model performed the best on most of the metrics. This result is also consistent with the `step` algorithm results. As such, this model was considered the \textbf{optimal model} for this problem.

```{r}
model.optimal <- model.5
```


### Optimal model analysis

The optimal model was then analyzed by studying a QQ-plot (see figure \ref{fig:qq-plot_optimal}) the squared standardized Pearson residuals and the standardized deviance residuals against the linear predictor $X\beta$ (see figure \ref{fig:residuals_optimal}). Furthermore the Cook’s distance was plotted against the linear predictor, `higrads` and against `region` (see figure \ref{fig:cooks_optimal}).


```{r, out.extra = '', fig.pos = "h", fig.width = 6, fig.height = 4, fig.cap="\\label{fig:qq-plot_optimal}QQ-plot for the optimal model"}
model.plot <- model.optimal
influence.plot <- influence(model.plot)
  
xb <- predict(model.plot)

r <- influence.plot$pear.res / sqrt(1 - influence.plot$hat)
ds <- influence.plot$dev.res / sqrt(1 - influence.plot$hat)

qqnorm(r)
qqline(r)

```
The QQ plot of the optimal model appear to follow a normal distribution, however skewed to the left.



\newpage

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:residuals_optimal}Standardized Pearson residuals as well as standardized deviance residuals for the optimal model, against the linear predictor $X\\beta$"}
par(mfrow = c(3, 1))

with(cdi, plot(r ~ xb, ylab = "Squared Pearson residuals", main = "Standardized Pearson residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

with(cdi, plot(r^2 ~ xb, ylab = "Squared Pearson residuals", main = "Squared standardized Pearson residuals against linear predictor"))
abline(h = c(0, 4), col = "red", lty = 3)

with(cdi, plot(ds ~ xb, ylab = "Deviance residuals", main = "Standardized deviance residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

outlier.index2 <- which(r^2 > 100)
outlier2 <- cdi[outlier.index2,]


```
There was one particular outlier that seem to generate an exceptionaly large Pearson residual. Furthermore there were a few Pearson Resdiuals that had a suspisiously large value. As for the devience residuals, some of them were too large as well. Disregarding the most extreme outlier, the residuals were not far worse off than the residuals of the interaction model. The most extreme outlier was the Olmsted county. The potential influence of individual observations was adressed by plotting the Cook's disatance, see figure \ref{fig:cooks_optimal}.   

\newpage

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:cooks_optimal}Cook's distance, for the optimal model, against linear predictor, region as well as higrads"}
par(mfrow = c(2, 2))

n <- nrow(cdi)
cook.plot <- cooks.distance(model.plot)

with(cdi, plot(cook.plot ~ xb, ylab = "Cook's distance", main = "Cook's distance against linear predictor"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ region, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ poors, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ phys1000, ylab = "Cook's distance", main = "Cook's distance against higrads"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

```

```{r, include = F}
outlier.index <- which(cook.plot > 0.4)
outlier <- cdi[outlier.index,]
outlier
```

The outlier in figure \ref{fig:cooks_optimal} was again `r outlier$county`, see figure \ref{fig:cooks_optimal_corrected} for plots of Cook's distance excluding this point. Table \ref{tab:comparison_optimal_corrected} compares the performance of these models.


\newpage

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:cooks_optimal_corrected}Cook's distance, for the optimal model, against linear predictor, region as well as higrads, excluding outlier Olmsted"}

cdi.corrected <- cdi[-c(outlier.index),]
model.optimal.corrected <- glm(hicrm ~ region + poors + phys1000, data = cdi.corrected, family = "binomial")
xb <- predict(model.optimal.corrected)

par(mfrow = c(2, 2))

n <- nrow(cdi)
cook.plot <- cooks.distance(model.optimal.corrected)

with(cdi.corrected, plot(cook.plot ~ xb, ylab = "Cook's distance", main = "Cook's distance against linear predictor"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi.corrected, plot(cook.plot ~ region, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi.corrected, plot(cook.plot ~ poors, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi.corrected, plot(cook.plot ~ phys1000, ylab = "Cook's distance", main = "Cook's distance against higrads"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

```
Removing the problematic data entry from the CDI data frame, and refitting the optimal model resulted in a substantial improvement in the the Cook's distance plots. Comparing the Cook's distsance plots and resiudal plots without the problematic data entry generated relatively simlar plots as the figure \ref{fig:cooks_interaction}. The main difference was the largest two outliers in the figure \ref{tab:comparison_optimal_corrected}.




 

```{r}
model.names <- c("Optimal", "Optimal excluding outlier")
models <- list(model.optimal, model.optimal.corrected)

compare.models <- calc.metrics.models(models, model.names)

compare.models$sensitivity <- compare.models$sensitivity * 100
compare.models$specificity <- compare.models$specificity * 100

col.names <- c("Model", "AIC", "BIC", "Sensitivity (%)", "Specificity (%)", "Pseudo R2")
table.caption <- "\\label{tab:comparison_optimal_corrected}Comparison of sensitivity and specificity of optimal model v.s. optimal model with outlier Olmsted removed"
kable(compare.models, caption = table.caption, col.names = col.names, digits = c(rep(0, 5), 2))
```

The model with the outlier removed performed even better.

### Discussion

In order to first get a view on the different covariates and how they relate to each other, they were plotted against eachother in figure \ref{fig:pairs}.

```{r, out.extra = '', fig.pos = "h", fig.width = 12, fig.height = 12, fig.cap="\\label{fig:pairs}Plot of covariates against eachother"}
ggpairs(cdi, columns = c("hicrm", "higrads", "poors", "phys1000", "region"), axisLabels = 'show', lower = list(combo = wrap("facethist", binwidth = 1)))
```

The optimal model included the previously studied covariate `region`, but discarded the `higrads` covariate. In addition, it included the new `poors` and `phys1000` covariates. One explaination why `higrads` was not used in the optimal may be seen in figure \ref{fig:pairs}, where there is a high correlation between `higrads` and `poors`. With increased multicolinearity the standrard error of the coefficents increase. In worst case this can cause some varibles to become insignificant. Problems such as these are not to unlikley to happen when involving both the `region` and `higrads` covariates, which have a correlation of -0.692 bewteen them. This is probably one of the reasons why the optimal model performed better than the interaction model, which had both the `higrads` and `region` as covarites, in the previous section.

This hypothesis was tested in figure \ref{fig:poors_higrads}, where a linear regression model between `poors` and `higrads` was been fit. Studying the P-value of the model revealed that the $\beta$-values were highly significant.

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 4, fig.cap="\\label{fig:poors_higrads}Plot of \\texttt{higrads} against \\texttt{poors}, together with linear regression line, with 95 \\% confidence and prediction intervals"}
plot(higrads ~ poors, data = cdi)
model.hp <- lm(higrads ~ poors, data = cdi)
sum <- summary(model.hp)

xx0 <- seq(0, 35, 0.1)  # just a grid of values for the predictor, from 23 to
# 38 with step 0.1
predx <- data.frame(poors = xx0)
# add the predictions and confidence interval as new columns in the predx data 
# frame, with name prefix "conf":
predx <- cbind(predx, conf = predict(model.hp, predx, interval = "confidence"))
# add the predictions (again) and prediction interval as new columns in the
# predx data frame, adding the prefix "pred" to the prediction columns:
predx <- cbind(predx, pred = predict(model.hp, predx, interval = "prediction"))
abline(model.hp, col = "blue")
# add confidence interval lines
with(predx, {
  lines(poors, conf.lwr, lty = 2, col = "red", lwd = 2)
  lines(poors, conf.upr, lty = 2, col = "red", lwd = 2)
})
# add prediction interval lines
with(predx, {
  lines(poors, pred.lwr, lty = 3, col = "blue", lwd = 2)
  lines(poors, pred.upr, lty = 3, col = "blue", lwd = 2)
})
```

Looking at how well `poors` predicts `hicrm` may be seen in figure \ref{fig:hicrm_poors}. Here it may be seen that `poors` follows a more distinct S-shape, and that `poors` seems to split the dataset more distinctly between high and non-high crime rate, as it varies from $\approx 15% - \approx 80%$, rather than the low separation discussed previously. As such, it seems that `higrads` and `poors` are highly correlated, but that `poors` better predict `hicrm` and is therefore better to use in the model.


```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 5, fig.cap="\\label{fig:hicrm_poors}Plot of \\texttt{hicrm} against \\texttt{poors}, including kernel smoothing and prediction of fitted model with 95 \\% confidence interval"}
with(cdi, {
   plot(hicrm ~ poors)
   lines(ksmooth(poors, hicrm, bandwidth = 6))
})

model.poors <- glm(hicrm ~ poors, data = cdi, family = "binomial")
#summary(model.poors)

x0 <- data.frame(poors = seq(0, 40, 1))

predx <- cbind(x0, prob = predict(model.poors, x0, type = "response"))
with(predx, lines(poors, prob, col = "blue"))

# calculate conf.int for the linear part x*beta:
standard.error <- 1.96
xb <- predict(model.poors, x0, se.fit = TRUE)
ci.xb <- data.frame(lwr = xb$fit - standard.error * xb$se.fit,
                    upr = xb$fit + standard.error * xb$se.fit)


# and finally CI for the probabilities and add to the plot:
predx <- cbind(predx, to.prob(exp(ci.xb)))
with(predx, {
  lines(poors, lwr, lty = 2, col = "red")
  lines(poors, upr, lty = 2, col = "red")
})

```


\newpage
Regarding `phys1000`, it appears in \ref{fig:pairs} that it does not have an as clear relationship to the other covariates and therefore provides more information to the model. Looking at how well `phys1000` predicts `hicrm`, seen in figure \ref{fig:hicrm_phys1000}, it seems to follow an approximate S-shape and therefor contributes to the model.



```{r code_plot, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 5, fig.cap="\\label{fig:hicrm_phys1000}Plot of \\texttt{hicrm} against \\texttt{phys1000}, including kernel smoothing and prediction of fitted model with 95 \\% confidence interval"}
with(cdi, {
   plot(hicrm ~ phys1000)
   lines(ksmooth(phys1000, hicrm, bandwidth = 2))
})

model.phys1000 <- glm(hicrm ~ phys1000, data = cdi, family = "binomial")
#summary(model.phys1000)

x0 <- data.frame(phys1000 = seq(0, 40, 1))

predx <- cbind(x0, prob = predict(model.phys1000, x0, type = "response"))
with(predx, lines(phys1000, prob, col = "blue"))

# calculate conf.int for the linear part x*beta:
standard.error <- 1.96
xb <- predict(model.phys1000, x0, se.fit = TRUE)
ci.xb <- data.frame(lwr = xb$fit - standard.error * xb$se.fit,
                    upr = xb$fit + standard.error * xb$se.fit)


# and finally CI for the probabilities and add to the plot:
predx <- cbind(predx, to.prob(exp(ci.xb)))
with(predx, {
  lines(phys1000, lwr, lty = 2, col = "red")
  lines(phys1000, upr, lty = 2, col = "red")
})

```


\newpage

To Summarize, the optimal model ignores the `higrads` covariate and use `region`, `poors` and `phys1000` as covariates. Discarding the `higrads` covarite probably generated a better model, because of the  problems associated with a high covariance between the `higrads` and `poors`. The low covarince among the covarites together with the fact that the `phys1000` and `poors` covarites are good at predicting `hicrm`, meant that the optimal model was better than the interaction model.     


