---
title: "Project 2"
author: "Axel Sjöberg & John Rapp Farnes"
date: "14 maj 2019"
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
classoption: a4paper
---

```{r setup_knitr, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  collapse = TRUE,
  comment = "#>"
)

options(tinytex.verbose = TRUE)
```

```{r setup_pdf, eval = FALSE}
# Install from CRAN
install.packages('rmarkdown')
# Render PDF
install.packages("tinytex")
tinytex::install_tinytex()  # install TinyTeX

```


```{r init_vars, include = FALSE}
library(ggplot2)
library(knitr)
library(dplyr)
library(purrr)
library(pscl)
library(GGally)

cdi <- read.delim("CDI.txt")
 
region.names <- c("Northeast", "Midwest", "South", "West")
cdi$region <- factor(cdi$region, levels = c(1, 2, 3, 4),
                     labels = region.names)
  
cdi$crm1000 <- 1000 * cdi$crimes / cdi$popul
cdi$phys1000 = 1000 * cdi$phys / cdi$popul


median = summary(cdi$crm1000)["Median"]

cdi <- cbind(cdi, hicrm = ifelse(cdi$crm1000 < median, 0, 1))
```


\newpage

# Introduction

## Background and dataset
The objective of this report is to determine which covarites that can be used to predict if a US county has a low or high crime rate (per 1000 inhabitants). The dataset used to do this was county demographic information (CDI) for 440 of the most populous counites in the US 1990-1992. Each county records includes data on the 14 variables listed below in table \ref{tab:cdi}. Counties with missing data has been removed from the dataset.


```{r}
variables = c("id", "county", "state", "area", "popul", "pop1834", "pop65plus", "phys", "beds", "crimes", "higrads", "bachelors", "poors", "unemployed", "percapitaincome", "totalincome", "region")

descriptions = c("identification number, 1–440", "county name", "state abbreviation", "land area (square miles)", "estimated 1990 population", "percent of 1990 CDI population aged 18–34", "percent of 1990 CDI population aged 65 years old or older", "number of professionally active nonfederal physicians during 1990", "total number of beds, cribs and bassinets during 1990", "total number of serious crimes in 1990", "percent of adults (25 yrs old or older) who completed at least 12 years of school", "percent of adults (25 yrs old or older) with bachelor’s degree", "Percent of 1990 CDI population with income below poverty level", "percent of 1990 CDI labor force which is unemployed", "per capita income of 1990 CDI population (dollars)", "total personal income of 1990 CDI population (in millions of dollars)", "Geographic region classification used by the U.S. Bureau of the Census,
                   including Northeast, Midwest, South and West")

variable.table = cbind(variables, descriptions)

kable(variable.table, caption = "\\label{tab:cdi}CDI dataset columns", col.names = c("Variable", "Description"))
```

In order to measure crime rate, another varible called `crm1000` was added to the data set, descibing the number of serious crimes per 1000 inhabitants. Using `crm1000`, counties were divided into counties with high or non-high crime rate, where counties with crime rate higher than the median of `crm1000` in the dataset were categorized as having a high crime rate. This crime status of the county was stored in another column called `hircrm`, which takes the value 1 if the county is a high crime county and zero if it is a low crime county. In this paper, this binary varible will be used as the dependent varible. Similar to crime rate, a covariate `phys1000` was also added, measuring the number of physicians per 1000 inhabitants.

## Model
The binary dependent variable was modelled using a logistic regression model. This model assumes that the log-odds of a certain observation $i$ is a linear combination of its covariates $X_{j,i}$ and parameters $\beta_i$. As such, the model looks like:
\begin{equation}
  \ln{\frac{p_i}{1 - p_i}} = \beta_0 + \sum_j\beta_{j} \cdot X_{j,i}
\end{equation}

# Analysis

## The higrad model

### Introduction

The model becomes

\begin{equation}
  \ln{\frac{p_i}{1 - p_i}} = \beta_0 + \beta_{higrads} \cdot X_{higrads,i} + \epsilon_i
\end{equation}

The first model considered had `higrads` as the sole covariate. In order to determine if there is a relationship between `hicrm` and `higrads` they are plotted against each other, see figure \ref{fig:hicrm_higrads}. Because `hicrm` is a binary varible it is very difficult to determine if there is a relationship by the pattern of the plot. In order to circumvent this problem a kernel smoother was added to the plot. The most smooth looking line was attained by setting the bandwidth to 20. The kernel curve was sort of S-shaped, implying that a logistic model may be appropriate. Further, the S-shape is "downward facing", implying a negative $\beta_{higrads}$, as the probability of a county being classified as a high crime county decreases when the amount of higrads in the county increses. Furthermore the fitted model along with its 95 % confidence interval was added to the plot in figure \ref{fig:hicrm_higrads}.


```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 5, fig.cap="\\label{fig:hicrm_higrads}Plot of \\texttt{hicrm} against \\texttt{higrads}, including kernel smoothing and prediction of fitted model with 95 \\% confidence interval"}
with(cdi, {
   plot(hicrm ~ higrads)
   lines(ksmooth(higrads, hicrm, bandwidth = 20))
})

model.higrads <- glm(hicrm ~ higrads, data = cdi, family = "binomial")
sum <- summary(model.higrads)

x0 <- data.frame(higrads = seq(0, 100, 1))

predx <- cbind(x0, prob = predict(model.higrads, x0, type = "response"))
with(predx, lines(higrads, prob, col = "blue"))

# calculate conf.int for the linear part x*beta:
standard.error <- 1.96
xb <- predict(model.higrads, x0, se.fit = TRUE)
ci.xb <- data.frame(lwr = xb$fit - standard.error * xb$se.fit,
                    upr = xb$fit + standard.error * xb$se.fit)

# transform to CI for the odds:
ci.odds <- exp(ci.xb)

to.prob = function(odds) {
  return (odds / (1 + odds))
}

# and finally CI for the probabilities and add to the plot:
predx <- cbind(predx, to.prob(ci.odds))
with(predx, {
  lines(higrads, lwr, lty = 2, col = "red")
  lines(higrads, upr, lty = 2, col = "red")
})

```


As can be seen in figure \ref{fig:hicrm_higrads}, a higher number of `higrads seems to make a county less probable to qualify as high crime county. Logically this makes sense. Tee $\beta$ values together with their 95 % confidence inteval is presented in table \ref{tab:higrad_beta}. Neither one of the $\beta$ confidence interval cover zero, meaning that they are statistically significant at $\alpha$ = 0.05. This is verfied by the very small P value. Something concerning about the plot in figure 1 is the few data point to the middle of the left, as they might influence predicted model in an erronious manner. Moreover the interval covered by the kernel smoother and the predicited model is quite low. 


### Fitted model and significance


```{r beta_confidence}
coeff.beta <- model.higrads$coefficients

ci.beta <- suppressMessages(confint(model.higrads))

beta.p <- sum$coefficients[, "Pr(>|z|)"]

table.beta.ci <- cbind("Estimate" = coeff.beta, ci.beta, "P-value" = beta.p)
row.names(table.beta.ci) <- c("$\\beta_0$", "$\\beta_{higrads}$")

table.caption <- "\\label{tab:higrad_beta}$\\beta$-values of \\texttt{higrad} model, with 95 \\% confidence inteval"
kable(table.beta.ci, caption = table.caption, digits = c(3,3,3,5))

```


```{r beta_decrease, include = F}
or.higrads <- exp(coeff.beta["higrads"])

decrease.one.percent <- 1 - or.higrads
decrease.ten.percent <- 1 - or.higrads^10

display.percent <- function(value) {
  return(round(value * 100, digits=1))
}

decrease.one.percent
decrease.ten.percent
```


If higrads increases 1%, odd decreases by `r display.percent(decrease.one.percent)`%
If higrads increases 10%, odd decreases by `r display.percent(decrease.ten.percent)`%

### Model predictions


Using the higrads model,the probability, with confidence interval, of having a high crime rate in a county
where the amount of higrads is 65 (percent), and where it is 85 (percent) is predicted. The result can be found in table 3.

```{r predict}
x0 <- data.frame(higrads = c(65, 85))


predx <- cbind(x0, prob = predict(model.higrads, x0, type = "response"))

xb <- predict(model.higrads, x0, se.fit = TRUE)
ci.xb <- data.frame(lwr = xb$fit - standard.error * xb$se.fit,
                    upr = xb$fit + standard.error * xb$se.fit)
# transform to CI for the odds:
ci.odds <- exp(ci.xb)

# and finally CI for the probabilities and add to the plot:
predx <- cbind(predx * 100, to.prob(ci.odds) * 100)

kable(predx, col.names = c("Higrads", "Probability (%)", "2.5 %", "97.5 %"), caption = "Test", digits = 1)
```

### Model performance analysis

```{r pred_sense_spec, include = F}

calc.sense.spec = function(model) {
  pred.counties <- cdi

  pred.counties <- cbind(pred.counties, prob.hicrm = predict(model, pred.counties, type = "response"))
  pred.counties <- cbind(pred.counties, pred.hicrm = ifelse(pred.counties$prob.hicrm > 0.5, 1, 0))
  
  num.hicrm <- sum(pred.counties$hicrm)
  
  # Sensitivity
  true.postive <- with(pred.counties, ifelse(pred.hicrm == 1 & hicrm == 1, 1, 0))
  sensitivity <- sum(true.postive) / num.hicrm
  sensitivity
  
  # Specificity
  true.negative <- with(pred.counties, ifelse(pred.hicrm == 0 & hicrm == 0, 1, 0))
  specificity <- sum(true.negative) / num.hicrm
  specificity
  
  metrics <- data.frame(
    sensitivity,
    specificity
  )
  
  return(metrics)
}

sense.spec.higrads <- calc.sense.spec(model.higrads)

sensitivity.higrads <- sense.spec.higrads$sensitivity
specificity.higrads <- sense.spec.higrads$specificity

```

In order to analyze model performance, the sensitivity and specificity of the model was calculated. The sensitivity of a model is the ratio of predicted positives to real positives in the dataset, while the specificity of a model is the ratio of predicted negatives to real negatives in the dataset. As such, the higher the value of the sensitivity and specificity, the better.

The sensitivity of the model was `display.percent(sensitivity.higrads)`% and `display.percent(specificity.higrads)` of the model was 57.3%. Thereby the higrad model does a rather bad job at correctly clasifying the the crime level status of the counties

## The region model

### Introduction

Next, a logistic model was adopted based on `region`. Since `region` is not continous , but categorial, it is modelled using "dummy variables" $X_i$. In order to implement this effectively, one of the categories is chosen as a reference variable, and the effects of other categories are measured in comparison to it.

In order to determine this reference variable - a cross-tabulation of the data between `region` and `hirm` is studied, see table \ref{tab:cross-tabulation}.

```{r cross_tabulation}

cross.table <- table(cdi %>% select(region, hicrm))

table.caption <- "\\label{tab:cross-tabulation}Cross-tabulation between \\texttt{region} and \\texttt{hicrm}"
kable(cross.table, col.names = c("Low crime", "High crime"), caption = table.caption)

reference.level <- "South"
cdi$region <- relevel(cdi$region, ref = reference.level)

```

As a reference region, the one that has the largest number of counties in it’s smallest low/high category was chosen. As a tie-breaker, the other low/high category was used. This approach produces the lowest standard error, and therefore highest significance. As seen in table \ref{tab:cross-tabulation}, the above given condition results in choosing `r reference.level` as reference region.


Using this reference region, the logistic model becomes
\begin{equation}
  \ln{\frac{p_i}{1 - p_i}} = \beta_0 + \beta_{Northeast} \cdot X_{Northeast,i} + \beta_{Midwest} \cdot X_{Midwest,i} + \beta_{West} \cdot X_{West,i} + \epsilon_i
\end{equation}

The $\beta$ coefficients are measured relative to `r reference.level` and $\beta_0$ is log-odds coefficient for `r reference.level`.

### Fitted model and significance

The model was fit with the given data set, estimating $\beta_i$, shown together with its 95 % confidence interval and P-value, in table \ref{tab:region_beta}.

```{r region_beta}
model.region <- glm(hicrm ~ region, data = cdi, family = "binomial")
sum <- summary(model.region)

coeff.beta <- model.region$coefficients

ci.beta <- suppressMessages(confint(model.region))

beta.p <- sum$coefficients[, "Pr(>|z|)"]

table.beta.ci <- cbind("Estimate" = coeff.beta, ci.beta, "P-value" = beta.p)
row.names(table.beta.ci) <- c("$\\beta_0$", "$\\beta_{Northeast}$", "$\\beta_{Midwest}$", "$\\beta_{West}$")

table.caption <- "\\label{tab:region_beta}$\\beta$-estimates for the \\texttt{region} model, together with 95 \\% confidence interval and P-values"
kable(table.beta.ci, caption = table.caption, digits = 3)
```

As may be seen in \ref{tab:region_beta}, the P-values for all of the $\beta$-estimates are not less than 0.05, indicating a lack of statistical significance on a 95 % level. $\beta_{west}$ is the only varible that has a P-value > 0.05. This means that the west region dummy varible might be irrelevant when having south as the reference category.

Next, the odds-ratios for the different categories where determined. The odds-ratios measure the odds of a particular category in relation to the reference category. These may be calculated as $OR_i = e^{\beta_i}$ and are seen in table \ref{tab:region_OR}.


```{r}
beta.region <- exp(coeff.beta)

table.beta.ci <- cbind("OR" = exp(coeff.beta), exp(ci.beta))
table.beta.ci <- table.beta.ci[-c(1), ]

row.names(table.beta.ci) <- region.names[region.names != reference.level]

table.caption <- "\\label{tab:region_OR}Odds-ratios for the \\texttt{region} model, together with 95 \\% confidence interval"
kable(table.beta.ci, caption = table.caption, digits = 2)


```

As seen in table \ref{tab:region_OR}, the odds-ratios are less than 1 for all categories but the reference region. This implies that the odds for all regions are lower compared to the reference region, i.e. that the probability of a high crime rate is lower in all regions compared to the reference region. This can also be seen in table \ref{tab:cross-tabulation}.



### Model predictions

Using the fitted model, the probabilies of having a high crime rate, with confidence interval, for the different regions was determined, shown in table \ref{tab:region_prob}.


```{r}


pred.counties <- subset(cdi, select = c(county, region, hicrm))

x0 <- data.frame(region = region.names)

pred <- predict(model.region, x0, se.fit = TRUE, type = "response")


## confidence interval for log-odds
ci.prob <- cbind("2.5 %" = pred$fit - standard.error * pred$se.fit, 
                "97.5 %" = pred$fit + standard.error * pred$se.fit)


table.prob.ci <- cbind("Probability (%)" = pred$fit * 100, ci.prob * 100)
row.names(table.prob.ci) <- region.names

table.caption <- "\\label{tab:region_prob}Probability of high crime rate (\\%), together with 95 \\% confidence interval for each of the regions"

kable(table.prob.ci, caption = table.caption, digits = 1)


```

### Model performance analysis


```{r, include = F}
sense.spec.region <- calc.sense.spec(model.region)

sensitivity.region <- sense.spec.region$sensitivity
specificity.region <- sense.spec.region$specificity

```

For the `region`, the sensitivity was `r display.percent(sensitivity.region)`%, while the specificity was `r display.percent(specificity.region)`%.


Comparing the two models analyzed so far, the `region` model performs better measured on sensitivity and specificity, as seen in table \ref{tab:compare_sense_spec_region_higrad}

```{r}

compare.sense <- data.frame(
  "Covariate" = c("Higrads", "Region"),
  "Sensitivity" = c(sensitivity.higrads, sensitivity.region) * 100,
  "Specificity" = c(specificity.higrads, specificity.region) * 100
);

table.caption <- "\\label{tab:compare_sense_spec_region_higrad}Comparison of sensitivity and specificity of \\texttt{higrad} and \\texttt{region} model"
kable(compare.sense, caption = table.caption, col.names = c("Covariate", "Sensitivity (%)", "Specificity (%)"), digits = 1)

```

## Combined model and comparison

### Introduction
Next a model that uses both `higrads` and `region` is analyzed. As such, this model becomes

\begin{equation}
  \ln{\frac{p_i}{1 - p_i}} = \beta_0 + \beta_{Northeast} \cdot X_{Northeast,i} + \beta_{Midwest} \cdot X_{Midwest,i} + \beta_{West} \cdot X_{West,i} + \beta_{higrads} \cdot X_{higrads,i} + \epsilon_i
\end{equation}

### Model comparison

In order to compare the models, metrics other than sensitivity and specificity may be studied. Some of these are AIC and BIC, which are penalized-likelihood criteria and Nagelkerke psuedo $R^2$, which is a measurment that increses up to 1 the better the model fit is. As they are defined, AIC and BIC should be as low as possible for a model to be performant, while psuedo $R^2$ should be as high as possible.

Comparison between the models in regards to AIC, BIC and Psuedo $R^2$ are seen in figure \ref{fig:comparison_region_higrads_both}, while comparison of sensitivity and specificity is seen in table \ref{tab:compare_sense_spec_region_higrad_both}.

```{r, include = F}
model.both <- glm(hicrm ~ higrads + region, data = cdi, family = "binomial")

model.names <- c("Higrads", "Region", "Both")
models <- list(model.higrads, model.region, model.both)

calc.metrics = function(model) {
  calc.aic <- AIC(model)
  calc.bic <- AIC(model, k = log(nrow(cdi)))
  sense.spec <- calc.sense.spec(model)
  
  pseudo.R2 <- pR2(model)["r2CU"]
  
  metrics <- data.frame(
    aic = calc.aic,
    bic = calc.bic,
    sensitivity = sense.spec$sensitivity,
    specificity = sense.spec$specificity,
    PseudoR2 = pseudo.R2
  )
  
  return (metrics)
}
calc.metrics.models = function(models, model.names) {
  models.metrics <- lapply(models, calc.metrics)

  compare.models <- do.call(rbind, models.metrics)
  compare.models <- cbind(model.name = model.names, compare.models)
  rownames(compare.models) <- NULL
  
  return (compare.models)  
}

compare.models <- calc.metrics.models(models, model.names)
compare.models
```



```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 5, fig.cap="\\label{fig:comparison_region_higrads_both}Comparison of AIC and BIC and Nagelkerke psuedo $R^2$ for the different models"}

n <- nrow(compare.models)
x <- 1:n
with(compare.models, {
  par(mar = c(5, 4, 4, 4) + 0.3)  # Leave space for second axis
  
  plot(x, rep(0, n), type = "n", ylim = c(500, 650), xaxt = "n",
       ylab = "AIC/BIC", xlab = "Model",
       main = "Model comparison")
  axis(1, at = x, labels = model.name)
  lines(x, aic, col = "red", lty = 2, lwd = 2)
  lines(x, bic, col = "red", lty = 1, lwd = 2)
  
  par(new = TRUE)
  plot(x, rep(0, n), type = "n", axes = FALSE, ylim = c(0, 1), xaxt = "n",
     xlab = "", ylab = "")
  lines(x, PseudoR2, col = "blue", lty = 1, lwd = 2)
  best.model <- 3
  axis(side=4, at = pretty(c(0, 1)))
  mtext("Pseudo R^2", side=4, line=3)
  
  legend("topright",legend=c("BIC", "AIC","Pseudo R^2"),
    text.col=c("red","red", "blue"), lty=c(1, 2, 1),col=c("red","red", "blue"))
  
})
```

```{r}
compare.sense <- compare.models %>% select(model.name, sensitivity, specificity)
compare.sense$sensitivity <- compare.models$sensitivity * 100
compare.sense$specificity <- compare.models$specificity * 100

table.caption <- "\\label{tab:compare_sense_spec_region_higrad_both}Comparison of sensitivity and specificity of models"
kable(compare.sense, caption = table.caption, col.names = c("Covariate", "Sensitivity (%)", "Specificity (%)"), digits = 1)

```

As seen in figure \ref{fig:comparison_region_higrads_both} and table \ref{tab:compare_sense_spec_region_higrad_both}, the combined model with both the covariates performs the best on all the studied metrics.

### Combined model performance

Performance of the combined model can be analyzed by studying a QQ-plot (see figure \ref{fig:qq-plot_combined}) the squared standardized Pearson residuals and the standardized deviance residuals against the linear predictor $x^{\beta}$ (see figure \ref{fig:residuals_combined}). As well as the Cook’s distance against the linear predictor, and against `higrads` and against `region` (see figure \ref{fig:cooks_combined}).


\newpage

```{r, out.extra = '', fig.pos = "h", fig.width = 6, fig.height = 4, fig.cap="\\label{fig:qq-plot_combined}QQ-plot for the combined model"}
model.plot <- model.both
influence.plot <- influence(model.plot)
  
xb <- predict(model.plot)

r <- influence.plot$pear.res / sqrt(1 - influence.plot$hat)
ds <- influence.plot$dev.res / sqrt(1 - influence.plot$hat)

qqnorm(r)
qqline(r)

```

The QQ-plot seem to follow a bimodal distribution.


```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:residuals_combined}Squared standardized Pearson residuals as well as standardized deviance residuals for the combined model, against the linear predictor $x^{\\beta}$"}
par(mfrow = c(2, 1))


with(cdi, plot(r ~ xb, ylab = "Squared Pearson residuals", main = "Squared standardized Pearson residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

with(cdi, plot(r^2 ~ xb, ylab = "Squared Pearson residuals", main = "Squared standardized Pearson residuals against linear predictor"))
abline(h = c(0, 4), col = "red", lty = 3)

with(cdi, plot(ds ~ xb, ylab = "Deviance residuals", main = "Standardized deviance residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

```
For a standardized pearson residual to be considered suspiciously large, $|r_i| > |\lambda_{\alpha/2}| ≈ 2$. This is true for 8 of the standadized pearson residuals, which can be seen in figures XX and XX, where the standardized pearson residuals are plotted against the linear predictor. For a deviance residual to be considered too large $|r_i| > 2 $. This is never the case which is verified by figure XX where the standardized devience residuals are plotted against the linear predictor. 

In order to measure how the $\beta$-estimates were influenced by indiviual observations Cook's distance for logistic regression was calcultaed and plotted. 


```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:cooks_combined}Cook's distance, for the combined model, against linear predictor, region as well as higrads"}
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))

n <- nrow(cdi)
cook.plot <- cooks.distance(model.plot)

with(cdi, plot(cook.plot ~ xb, ylab = "Cook's distance", main = "Cook's distance against linear predictor"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ region, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ higrads, ylab = "Cook's distance", main = "Cook's distance against higrads"))
abline(h = c(1, 4 / n), col = "red", lty = 2)


```

As can be seen in diagrams XX, XX, XX the vast majoriy of the the observations is below the horizontal line. The amount of counties with a high Cook's distance is limited to 3-4. The data entry with the largest Cook's distance is found in the South region while the rest of the counties with a high Cook's distance is found in the West region. 



Anything alarmin?
Any interesting finds?

## Interaction model
### Introduction
As a forth model, interaction terms are also considered, building in that the effect of higrads may be different in different regions, where the log-odds in the model includes interaction terms such as $\beta_{Northeast * higrads} \cdot X_{higrads,i}$.


### Model performance

```{r, include = F}
model.interaction <- glm(hicrm ~ higrads * region, data = cdi, family = "binomial")

anova.combined.interaction <- anova(model.interaction, model.both, test = "LRT")
anova.combined.interaction
likelihood.p_value <- anova.combined.interaction$`Pr(>Chi)`[2]

```


The performance of the interaction model compared to the combined model may be analyzed by the likelihood test. This test is similar to a partial F-test, but adapted for logistical regression, using likelihoods since sums of squares are not applicable. The likelihood test results in the interaction model being significantly better than the combined model, with a P-value of `r round(likelihood.p_value, 3)`.

In addition, the AIC, BIC, Nagelkerke, sensitivity and specificity is compared to the combined model, in table \ref{tab:compare_sense_spec_interaction}. Performance of the interaction model can be analyzed by studying a QQ-plot (see figure \ref{fig:qq-plot_interaction}) the squared standardized Pearson residuals and the standardized deviance residuals against the linear predictor $x^{\beta}$ (see figure \ref{fig:residuals_interaction}). As well as the Cook’s distance against the linear predictor, and against `higrads` and against `region` (see figure \ref{fig:cooks_interaction}).


```{r}

models <- list(model.interaction, model.both)
model.names <- c("Combined model", "Interaction model")

compare.models <- calc.metrics.models(models, model.names)

compare.models$sensitivity <- compare.models$sensitivity * 100
compare.models$specificity <- compare.models$specificity * 100

col.names <- c("Covariate", "AIC", "BIC", "Sensitivity (%)", "Specificity (%)", "Pseudo R2")
table.caption <- "\\label{tab:compare_sense_spec_interaction}Comparison of sensitivity and specificity of models"
kable(compare.models, caption = table.caption, col.names = col.names, digits = c(rep(0, 5), 2))


```


\newpage


```{r, out.extra = '', fig.pos = "h", fig.width = 6, fig.height = 4, fig.cap="\\label{fig:qq-plot_interaction}QQ-plot for the integration model"}
model.plot <- model.both
influence.plot <- influence(model.plot)
  
xb <- predict(model.plot)

r <- influence.plot$pear.res / sqrt(1 - influence.plot$hat)
ds <- influence.plot$dev.res / sqrt(1 - influence.plot$hat)

qqnorm(r)
qqline(r)

```

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:residuals_interaction}Squared standardized Pearson residuals as well as standardized deviance residuals for the interaction model, against the linear predictor $x^{\\beta}$"}
par(mfrow = c(2, 1))

with(cdi, plot(r^2 ~ xb, ylab = "Squared Pearson residuals", main = "Squared standardized Pearson residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

with(cdi, plot(ds ~ xb, ylab = "Deviance residuals", main = "Standardized deviance residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

```

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:cooks_interaction}Cook's distance, for the interaction model, against linear predictor, region as well as higrads"}
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE))

n <- nrow(cdi)
cook.plot <- cooks.distance(model.plot)

with(cdi, plot(cook.plot ~ xb, ylab = "Cook's distance", main = "Cook's distance against linear predictor"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ region, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ higrads, ylab = "Cook's distance", main = "Cook's distance against higrads"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

```

Both models perform better on some metrics, while performing worse on other. The interaction model has a worse AIC-value, but a lower BIC-value (KOMMENTERA). The sensitivity, specificity and Pseudo $R^2$ values are worse for the interaction model.

NÅGOT OM COOKS MM?

VILKEN ÄR BÄST?

## Finding the optimal model

### Methology

Next, an attempt to fit an optimal model to predict high crime rates is made, using the previous covariates, as well as `poors` and `pshys1000`. Interaction terms are ignored.

Models of increasingly complexity, adding more covariates are compared to each other on the used metrics, i.e. AIC, BIC, Pseudo $R^2$, sensitivity and specificity. In addition, the result of automatic selection using R `step` function is studyied.

### Model comparison

AIC, BIC and Pseudo $R^2$ of the studied model are shown in figure \ref{fig:comparison_optimal}. In addition, table \ref{tab:comparison_optimal} includes sensitivity and specificity for the different models. 
```{R, include = F}
model.1 <- glm(hicrm ~ higrads, data = cdi, family = "binomial")
model.2 <- glm(hicrm ~ higrads + region, data = cdi, family = "binomial")
model.3 <- glm(hicrm ~ higrads + region + poors, data = cdi, family = "binomial")
model.4 <- glm(hicrm ~ higrads + region + poors + phys1000, data = cdi, family = "binomial")

model.5 <- glm(hicrm ~ region + poors + phys1000, data = cdi, family = "binomial")
model.6 <- glm(hicrm ~ higrads + region + phys1000, data = cdi, family = "binomial")


model.names <- c("H", "H + R", "H + R + Po", "H + R + Po + Phy", "R + Po + Phy", "H + R + Phy")
models <- list(model.1, model.2, model.3, model.4, model.5, model.6)

compare.models <- calc.metrics.models(models, model.names)

compare.models

step(model.4, k = log(nrow(cdi)))

anova(model.4, model.5, test = "LRT")
```

```{r, out.extra = '', fig.pos = "h", fig.width = 10, fig.height = 5, fig.cap="\\label{fig:comparison_optimal}Comparison of AIC and BIC and Nagelkerke psuedo $R^2$ for the different models. Key: H = \\texttt{higrads}, R = \\texttt{region}, Po = \\texttt{poors}, Phy = \\texttt{phys1000}"}

n <- nrow(compare.models)
x <- 1:n
with(compare.models, {
  par(mar = c(5, 4, 4, 4) + 0.3)  # Leave space for second axis
  
  plot(x, rep(0, n), type = "n", ylim = c(450, 650), xaxt = "n",
       ylab = "AIC/BIC", xlab = "Model",
       main = "Model comparison")
  axis(1, at = x, labels = model.name)
  lines(x, aic, col = "red", lty = 2, lwd = 2)
  lines(x, bic, col = "red", lty = 1, lwd = 2)
  
  par(new = TRUE)
  plot(x, rep(0, n), type = "n", axes = FALSE, ylim = c(0, 1), xaxt = "n",
     xlab = "", ylab = "")
  lines(x, PseudoR2, col = "blue", lty = 1, lwd = 2)
  best.model <- 3
  axis(side=4, at = pretty(c(0, 1)))
  mtext("Pseudo R^2", side=4, line=3)
  
  legend("topright",legend=c("BIC", "AIC","Pseudo R^2"),
    text.col=c("red","red", "blue"), lty=c(1, 2, 1), col=c("red","red", "blue"))
})

```

```{r}

compare.models$sensitivity <- compare.models$sensitivity * 100
compare.models$specificity <- compare.models$specificity * 100

col.names <- c("Model", "AIC", "BIC", "Sensitivity (%)", "Specificity (%)", "Pseudo R2")
table.caption <- "\\label{tab:comparison_optimal}Comparison of sensitivity and specificity of models. Key: H = \\texttt{higrads}, R = \\texttt{region}, Po = \\texttt{poors}, Phy = \\texttt{phys1000}"
kable(compare.models, caption = table.caption, col.names = col.names, digits = c(rep(0, 5), 2))

```

The results in \ref{fig:comparison_optimal} and \ref{tab:comparison_optimal} show that the `region` + `poors` + `phys1000` model performs best on most of the metrics. This result is also consistent with the `step` algorithm results. As such, this model is considered the \textbf{optimal model} for this problem.

```{r}
model.optimal <- model.5
```


### Model performance

Performance of the optimal model is then analyzed by studying a QQ-plot (see figure \ref{fig:qq-plot_optimal}) the squared standardized Pearson residuals and the standardized deviance residuals against the linear predictor $x^{\beta}$ (see figure \ref{fig:residuals_optimal}). As well as the Cook’s distance against the linear predictor, and against `higrads` and against `region` (see figure \ref{fig:cooks_optimal}).


\newpage


```{r, out.extra = '', fig.pos = "h", fig.width = 6, fig.height = 4, fig.cap="\\label{fig:qq-plot_optimal}QQ-plot for the optimal model"}
model.plot <- model.optimal
influence.plot <- influence(model.plot)
  
xb <- predict(model.plot)

r <- influence.plot$pear.res / sqrt(1 - influence.plot$hat)
ds <- influence.plot$dev.res / sqrt(1 - influence.plot$hat)

qqnorm(r)
qqline(r)

```

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:residuals_optimal}Squared standardized Pearson residuals as well as standardized deviance residuals for the optimal model, against the linear predictor $x^{\\beta}$"}
par(mfrow = c(2, 1))

with(cdi, plot(r^2 ~ xb, ylab = "Squared Pearson residuals", main = "Squared standardized Pearson residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

with(cdi, plot(ds ~ xb, ylab = "Deviance residuals", main = "Standardized deviance residuals against linear predictor"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)

```

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:cooks_optimal}Cook's distance, for the optimal model, against linear predictor, region as well as higrads"}
par(mfrow = c(2, 2))

n <- nrow(cdi)
cook.plot <- cooks.distance(model.plot)

with(cdi, plot(cook.plot ~ xb, ylab = "Cook's distance", main = "Cook's distance against linear predictor"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ region, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ poors, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi, plot(cook.plot ~ phys1000, ylab = "Cook's distance", main = "Cook's distance against higrads"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

```

```{r, include = F}
outlier.index <- which(cook.plot > 0.4)
outlier <- cdi[outlier.index,]
outlier
```

The outlier is `r outlier$county`, see figure \ref{fig:cooks_optimal_corrected} for a plot of Cook's distance excluding this point. Table \ref{tab:comparison_optimal_corrected} compares the performance of these models.

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 8, fig.cap="\\label{fig:cooks_optimal_corrected}Cook's distance, for the optimal model, against linear predictor, region as well as higrads, excluding outlier Olmsted"}

cdi.corrected <- cdi[-c(outlier.index),]
model.optimal.corrected <- glm(hicrm ~ region + poors + phys1000, data = cdi.corrected, family = "binomial")
xb <- predict(model.optimal.corrected)

par(mfrow = c(2, 2))

n <- nrow(cdi)
cook.plot <- cooks.distance(model.optimal.corrected)

with(cdi.corrected, plot(cook.plot ~ xb, ylab = "Cook's distance", main = "Cook's distance against linear predictor"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi.corrected, plot(cook.plot ~ region, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi.corrected, plot(cook.plot ~ poors, ylab = "Cook's distance", main = "Cook's distance against region"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

with(cdi.corrected, plot(cook.plot ~ phys1000, ylab = "Cook's distance", main = "Cook's distance against higrads"))
abline(h = c(1, 4 / n), col = "red", lty = 2)

```

```{r}
model.names <- c("Optimal", "Optimal without outlier")
models <- list(model.optimal, model.optimal.corrected)

compare.models <- calc.metrics.models(models, model.names)

compare.models$sensitivity <- compare.models$sensitivity * 100
compare.models$specificity <- compare.models$specificity * 100

col.names <- c("Model", "AIC", "BIC", "Sensitivity (%)", "Specificity (%)", "Pseudo R2")
table.caption <- "\\label{tab:comparison_optimal_corrected}Comparison of sensitivity and specificity of optimal model v.s. optimal model with outlier Olmsted removed"
kable(compare.models, caption = table.caption, col.names = col.names, digits = c(rep(0, 5), 2))
```

The model with the outlier removed performs better.

### Discussion

In order to first get a view on the different covariates and how they relate to each other, they are plotted against eachother in figure \ref{fig:pairs}.

```{r, out.extra = '', fig.pos = "h", fig.width = 12, fig.height = 12, fig.cap="\\label{fig:pairs}Plot of covariates against eachother"}
ggpairs(cdi, columns = c("hicrm", "higrads", "poors", "phys1000", "region"), axisLabels = 'show')
```

The optimal model includes the previously studied covariate `region`, but discards the `higrads` covariate. In addition, it includes the new `poors` and `phys1000` covariates. One explaination why `higrads` is not used in the optimal may be seen in figure \ref{fig:pairs}, where there is a high correlation between `higrads` and `poors`. This means that a large part of the correlation between higrads and hicrm is better the effect higrads    

This hypothesis is tested in figure \ref{fig:poors_higrads}, where a linear regression model has been fit. Studying the P-value of the model reveals that the $\beta$-values are highly significant.
```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 4, fig.cap="\\label{fig:poors_higrads}Plot of \\texttt{higrads} against \\texttt{poors}, together with linear regression line, with 95 % confidence and prediction intervals"}
plot(higrads ~ poors, data = cdi)
model.hp <- lm(higrads ~ poors, data = cdi)
sum <- summary(model.hp)

xx0 <- seq(0, 35, 0.1)  # just a grid of values for the predictor, from 23 to
# 38 with step 0.1
predx <- data.frame(poors = xx0)
# add the predictions and confidence interval as new columns in the predx data 
# frame, with name prefix "conf":
predx <- cbind(predx, conf = predict(model.hp, predx, interval = "confidence"))
# add the predictions (again) and prediction interval as new columns in the
# predx data frame, adding the prefix "pred" to the prediction columns:
predx <- cbind(predx, pred = predict(model.hp, predx, interval = "prediction"))
abline(model.hp, col = "blue")
# add confidence interval lines
with(predx, {
  lines(poors, conf.lwr, lty = 2, col = "red", lwd = 2)
  lines(poors, conf.upr, lty = 2, col = "red", lwd = 2)
})
# add prediction interval lines
with(predx, {
  lines(poors, pred.lwr, lty = 3, col = "blue", lwd = 2)
  lines(poors, pred.upr, lty = 3, col = "blue", lwd = 2)
})
```

Looking at how well `poors` predicts `hicrm` may be seen in figure \ref{fig:hicrm_poors}. Here it may be seen that `poors` follow a more distinct S-shape, and that `poors` seems to split the dataset more distinctly between high and non-high crime rate, as it varies from $\approx 15% - \approx 80%$, rather than the low separation discussed previously. As such,it seems that `higrads` and `poors` are highly correlated, but that `poors` better predict `hicrm` and is therefore better left in the model.

```{r, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 5, fig.cap="\\label{fig:hicrm_poors}Plot of \\texttt{hicrm} against \\texttt{poors}, including kernel smoothing and prediction of fitted model with 95 \\% confidence interval"}
with(cdi, {
   plot(hicrm ~ poors)
   lines(ksmooth(poors, hicrm, bandwidth = 6))
})

model.poors <- glm(hicrm ~ poors, data = cdi, family = "binomial")
#summary(model.poors)

x0 <- data.frame(poors = seq(0, 40, 1))

predx <- cbind(x0, prob = predict(model.poors, x0, type = "response"))
with(predx, lines(poors, prob, col = "blue"))

# calculate conf.int for the linear part x*beta:
standard.error <- 1.96
xb <- predict(model.poors, x0, se.fit = TRUE)
ci.xb <- data.frame(lwr = xb$fit - standard.error * xb$se.fit,
                    upr = xb$fit + standard.error * xb$se.fit)


# and finally CI for the probabilities and add to the plot:
predx <- cbind(predx, to.prob(exp(ci.xb)))
with(predx, {
  lines(poors, lwr, lty = 2, col = "red")
  lines(poors, upr, lty = 2, col = "red")
})

```

Regarding `phys1000`, it appears in \ref{fig:pairs} that it does not have an as clear relationship to the other covariates and therefore provides more information to the model. Looking at how well `phys1000` predicts `hicrm`, seen in figure \ref{fig:hicrm_phys1000}, it seems to follow an approximate S-shape and therefor contributes to the model.

```{r code_plot, out.extra = '', fig.pos = "h", fig.width = 8, fig.height = 5, fig.cap="\\label{fig:hicrm_phys1000}Plot of \\texttt{hicrm} against \\texttt{phys1000}, including kernel smoothing and prediction of fitted model with 95 \\% confidence interval"}
with(cdi, {
   plot(hicrm ~ phys1000)
   lines(ksmooth(phys1000, hicrm, bandwidth = 2))
})

model.phys1000 <- glm(hicrm ~ phys1000, data = cdi, family = "binomial")
#summary(model.phys1000)

x0 <- data.frame(phys1000 = seq(0, 40, 1))

predx <- cbind(x0, prob = predict(model.phys1000, x0, type = "response"))
with(predx, lines(phys1000, prob, col = "blue"))

# calculate conf.int for the linear part x*beta:
standard.error <- 1.96
xb <- predict(model.phys1000, x0, se.fit = TRUE)
ci.xb <- data.frame(lwr = xb$fit - standard.error * xb$se.fit,
                    upr = xb$fit + standard.error * xb$se.fit)


# and finally CI for the probabilities and add to the plot:
predx <- cbind(predx, to.prob(exp(ci.xb)))
with(predx, {
  lines(phys1000, lwr, lty = 2, col = "red")
  lines(phys1000, upr, lty = 2, col = "red")
})

```




