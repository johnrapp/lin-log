---
title: "Project 2"
author: "Axel SjÃ¶berg & John Rapp Farnes"
date: "8 maj 2019"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
classoption: a4paper
---

```{r setup_knitr, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup_pdf, eval = FALSE}
# Install from CRAN
install.packages('rmarkdown')

# Render PDF
install.packages("tinytex")
tinytex::install_tinytex()  # install TinyTeX

```


```{r init_vars, include = FALSE}
library(ggplot2)
library(knitr)
library(dplyr)
#library(rlist)
library(purrr)
library(pscl)

cdi <- read.delim("CDI.txt")
 
region.names <- c("Northeast", "Midwest", "South", "West")
cdi$region <- factor(cdi$region, levels = c(1, 2, 3, 4),
                     labels = region.names)
  
cdi$crm1000 <- 1000 * cdi$crimes / cdi$popul
cdi$crm1000 <- 1000 * cdi$crimes / cdi$popul
cdi$phys1000 = 1000 * cdi$phys / cdi$popul


median = summary(cdi$crm1000)["Median"]

cdi <- cbind(cdi, hicrm = ifelse(cdi$crm1000 < median, 0, 1))
```


\newpage


Structure:
Introduction:
  Uppgiften
  Dataset
  Modellen
  
Analysis:
  Higrad model:
    Introduction
    Results
  Region model:
  Integration model:
  Determining best model:

# Introduction

Background 

The objective of this report was to determine which covarites that can be used to predict if a US county has a low or high crime rate (per 1000 inhabitants). Data used to do this was county demographic information (CDI) for 440 of the most populous counites in the US 1990-1992. The data entries contains information on the 14 varibles listed below, for each of the counties. The data has been sorted so that counties with missing data has been removed from the dataset.  


```{r}
variables = t(cbind("id", "county", "state", "area", "popul", "pop1834", "pop65plus", "phys", "beds", "crimes", "higrads", "bachelors", "poors", "unemployed", "percapitaincome", "totalincome", "region"))

descriptions = t(cbind("identification number, 1–440", "county name", "state abbreviation", "land area (square miles)", "estimated 1990 population", "percent of 1990 CDI population aged 18–34", "percent of 1990 CDI population aged 65 years old or older", "number of professionally active nonfederal physicians during 1990", "total number of beds, cribs and bassinets during 1990", "total number of serious crimes in 1990 (including murder, rape, robbery,
aggravated assault, burglary, larceny-theft, motor vehicle theft)", "percent of adults (25 yrs old or older) who completed at least 12 years of school", "percent of adults (25 yrs old or older) with bachelor’s degree", "Percent of 1990 CDI population with income below poverty level", "percent of 1990 CDI labor force which is unemployed", "per capita income of 1990 CDI population (dollars)", "total personal income of 1990 CDI population (in millions of dollars)", "Geographic region classification used by the U.S. Bureau of the Census, where
1 = Northeast, 2 = Midwest, 3 = South, 4 = West"))
variable.table = cbind(variables, descriptions)

kable(variable.table, caption = "CDI varibles", col.names = c("Variable", "Description"))
```

In order to account for size, another varible called crm1000 was added to the data set. crm1000 contains the number of serious crimes per 1000 inhabitants. With the use of the median of this varible the crime rate of a county is classified as high if it is above the median and classified as low if it is below the median. This crime status of the county was stored in another varible called hircrm. In this paper, this categorical varible will be used as the dependent varible. hicrm takes the value 1 if the county is a high crime county and zero if it is a low crime county.



# Analysis

## Predicting crime rate based on education level

### Seeing if there is relationship

We saw if there was a relationship by plotting bla bla with kernel smoother.


```{r code_plot, fig.width = 8, fig.height = 5, fig.cap = "Bla bla", fig.pos = "h", out.extra = ''}
with(cdi, {
   plot(hicrm ~ higrads)
   lines(ksmooth(higrads, hicrm, bandwidth = 20))
})

model.higrads <- glm(hicrm ~ higrads, data = cdi, family = "binomial")
sum <- summary(model.higrads)

x0 <- data.frame(higrads = seq(0, 100, 1))

predx <- cbind(x0, prob = predict(model.higrads, x0, type = "response"))
with(predx, lines(higrads, prob, col = "blue"))

# calculate conf.int for the linear part x*beta:
standard.error <- 1.96
xb <- predict(model.higrads, x0, se.fit = TRUE)
ci.xb <- data.frame(lwr = xb$fit - standard.error * xb$se.fit,
                    upr = xb$fit + standard.error * xb$se.fit)

# transform to CI for the odds:
ci.odds <- exp(ci.xb)

# and finally CI for the probabilities and add to the plot:
predx <- cbind(predx, ci.odds / (1 + ci.odds))
with(predx, {
  lines(higrads, lwr, lty = 2, col = "red")
  lines(higrads, upr, lty = 2, col = "red")
})

```

Seems to be relationship!

KOLLA UTAN BÖRJAN


### Confidence intervals and significance

Report the beta-estimates together with their confidence intervals and test whether the amount of adults with
12 years in school has a significant effect on the probability of having a higher than median crime rate


```{r beta_confidence}
to.prob = function(odds) {
  return (odds / (1 + odds))
}

coeff.beta <- model.higrads$coefficients

ci.beta <- suppressMessages(confint(model.higrads))

beta.p <- sum$coefficients[, "Pr(>|z|)"] * 100

table.beta.ci <- cbind("Estimate" = coeff.beta, ci.beta, "P-value (%)" = beta.p)
row.names(table.beta.ci) <- c("$\\beta_0$", "$\\beta_1$")

kable(table.beta.ci, caption = "Test", digits = 3)

```


Significance!

### Change in odds

Estimate the relative change in the odds (odds ratio) of having a high crime rate, with confidence interval,
when the amount of higrads is increased by 1 (percent), and when it is increased by 10 (percent).

```{r beta_decrease, include = F}
beta.higrads <- exp(coeff.beta["higrads"])

decrease.one.percent <- 1 - beta.higrads
decrease.ten.percent <- 1 - beta.higrads^10

display.percent <- function(value) {
  return(round(value * 100, digits=1))
}

decrease.one.percent
decrease.ten.percent
```

If higrads increases 1%, odd decreases by `r display.percent(decrease.one.percent)`%
If higrads increases 10%, odd decreases by `r display.percent(decrease.ten.percent)`%


### Predict

Use the model to predict the probability, with confidence interval, of having a high crime rate in a county
where the amount of higrads is 65 (percent), and where it is 85 (percent).
```{r predict}
x0 <- data.frame(higrads = c(65, 85))


predx <- cbind(x0, prob = predict(model.higrads, x0, type = "response"))

xb <- predict(model.higrads, x0, se.fit = TRUE)
ci.xb <- data.frame(lwr = xb$fit - standard.error * xb$se.fit,
                    upr = xb$fit + standard.error * xb$se.fit)
# transform to CI for the odds:
ci.odds <- exp(ci.xb)

# and finally CI for the probabilities and add to the plot:
predx <- cbind(predx, to.prob(ci.odds))

kable(predx, col.names = c("Higrads", "Probability", "2.5 %", "97.5 %"), caption = "Test")
```

Use the model to predict, for each of the counties, whether it would be expected to have a low or a high crime 
rate (predicted probability below or above 0.5) and calculate the sensitivity and specificity for this model.

Sensitivity is the proportion of the true successes that have been correctly classified as successes (true positive).
Specificity is the proportion of the true failures that have been correctly classified as failures (true negatives).


```{r pred_sense_spec, include = F}

calc.sense.spec = function(model) {
  pred.counties <- cdi

  pred.counties <- cbind(pred.counties, prob.hicrm = predict(model, pred.counties, type = "response"))
  pred.counties <- cbind(pred.counties, pred.hicrm = ifelse(pred.counties$prob.hicrm > 0.5, 1, 0))
  
  num.hicrm <- sum(pred.counties$hicrm)
  
  # Sensitivity
  true.postive <- with(pred.counties, ifelse(pred.hicrm == 1 & hicrm == 1, 1, 0))
  sensitivity <- sum(true.postive) / num.hicrm
  sensitivity
  
  # Specificity
  true.negative <- with(pred.counties, ifelse(pred.hicrm == 0 & hicrm == 0, 1, 0))
  specificity <- sum(true.negative) / num.hicrm
  specificity
  
  metrics <- data.frame(
    sensitivity,
    specificity
  )
  
  return(metrics)
}

sense.spec.higrads <- calc.sense.spec(model.higrads)

sensitivity.higrads <- sense.spec.higrads$sensitivity
specificity.higrads <- sense.spec.higrads$specificity

```

Sensitivity was `r display.percent(sensitivity.higrads)`%

Specificity was `r display.percent(specificity.higrads)`%

## Predicting crime rate based on region

Make a cross-tabulation between region and hicrm. Choose as reference region in your regression models
the one that has the largest number of counties in it’s smallest low/high category. As a tie-breaker, use the
other low/high category. Why is this a good idea? Hint: look at how the standard errors for the log odds
(ratios) are calculated in this situation.

```{r cross_tabulation}

cross.table <- table(cdi %>% select(region, hicrm))

kable(cross.table, col.names = c("Low crime", "High crime"), caption = "Test")

reference.level <- "South"

#prop.table(cross.table)
#prop.table(cross.table, 1)
#prop.table(cross.table, 2)

```

Set the reference level to `r reference.level`, makes sense to get low SE



Fit a logistic regression using region as (categorical) covariate and report the beta-estimates together with their
confidence intervals. Test whether there are any significant differences between the regions in the probability
of having a high crime rate.

```{r region_beta}
# Set reference
cdi$region <- relevel(cdi$region, ref = reference.level)

model.region <- glm(hicrm ~ region, data = cdi, family = "binomial")

coeff.beta <- model.region$coefficients

ci.beta <- suppressMessages(confint(model.region))

beta.p <- sum$coefficients[, "Pr(>|z|)"] * 100

table.beta.ci <- cbind("Estimate" = coeff.beta, ci.beta, "P-value (%)" = beta.p)
row.names(table.beta.ci) <- c("$\\beta_0$", "$\\beta_1$", "$\\beta_2$", "$\\beta_3$")

kable(table.beta.ci, caption = "Test", digits = 3)

```

Significant!


Estimate the odds ratios for having a high crime rate, with confidence interval, for the different regions,
compared to the reference region.



```{r beta_decreases}
beta.region <- exp(coeff.beta)

table.beta.ci <- cbind("OR" = exp(coeff.beta), exp(ci.beta))
table.beta.ci <- table.beta.ci[-c(1), ]

row.names(table.beta.ci) <- region.names[region.names != reference.level]

kable(table.beta.ci, caption = "Test", digits = 1)


```


Also estimate the probability of having a high crime rate, with confidence interval, for the different regions, including the reference region.


Calculate the sensitivity and specificity for this model. If we are allowed to have either higrads or region
as covariate, which one should we choose?


```{r pred_sense_spec2}


pred.counties <- subset(cdi, select = c(county, region, hicrm))

x0 <- data.frame(region = region.names)

pred <- predict(model.region, x0, se.fit = TRUE, type = "response")


## confidence interval for log-odds
ci.prob <- cbind("2.5 %" = pred$fit - standard.error * pred$se.fit, 
                "97.5 %" = pred$fit + standard.error * pred$se.fit)


table.prob.ci <- cbind("Estimate" = pred$fit * 100, ci.prob * 100)
row.names(table.prob.ci) <- region.names

kable(table.prob.ci, caption = "Test", digits = 1)


```

```{r pred_sense_spec3, include = F}
sense.spec.region <- calc.sense.spec(model.region)

sensitivity.region <- sense.spec.region$sensitivity
specificity.region <- sense.spec.region$specificity

```

Sensitivity was `r display.percent(sensitivity.region)`%

Specificity was `r display.percent(specificity.region)`%

```{r pred_sense_spec4}

compare.sense <- data.frame(
  "Covariate" = c("Higrads", "Region"),
  "Sensitivity" = c(sensitivity.higrads, sensitivity.region),
  "Specificity" = c(specificity.higrads, specificity.region)
);
kable(compare.sense, caption = "Test", digits = 2)

```

Would prefer region

## Predicting crime rate based on region and higrads

Fit a third model using both higrads and region. For all three models, report their AIC, BIC, Nagelkerke
pseudo R^2, sensitivity and specificity. Which model is best?

```{r, include=F}
model.both <- glm(hicrm ~ higrads + region, data = cdi, family = "binomial")

model.names <- c("Higrads", "Region", "Both")
models <- list(model.higrads, model.region, model.both)

calc.metrics = function(model) {
  calc.aic <- AIC(model)
  calc.bic <- AIC(model, k = log(nrow(cdi)))
  sense.spec <- calc.sense.spec(model)
  
  pseudo.R2 <- pR2(model)["r2CU"]
  
  metrics <- data.frame(
    aic = calc.aic,
    bic = calc.bic,
    sensitivity = sense.spec$sensitivity,
    specificity = sense.spec$specificity,
    PseudoR2 = pseudo.R2
  )
  
  return (metrics)
}
calc.metrics.models = function(models, model.names) {
  models.metrics <- lapply(models, calc.metrics)

  compare.models <- do.call(rbind, models.metrics)
  compare.models <- cbind(model.name = model.names, compare.models)
  rownames(compare.models) <- NULL
  
  return (compare.models)  
}

compare.models <- calc.metrics.models(models, model.names)
compare.models
```



### Comparing models
```{r, fig.width = 8, fig.height = 5, fig.cap = "Bla bla", fig.pos = "h", out.extra = ''}

n <- nrow(compare.models)
x <- 1:n
with(compare.models, {
  par(mar = c(5, 4, 4, 4) + 0.3)  # Leave space for second axis
  
  plot(x, rep(0, n), type = "n", ylim = c(500, 650), xaxt = "n",
       ylab = "AIC/BIC", xlab = "Model",
       main = "Model comparison")
  axis(1, at = x, labels = model.name)
  lines(x, aic, col = "red", lty = 2, lwd = 2)
  lines(x, bic, col = "red", lty = 1, lwd = 2)
  
  par(new = TRUE)
  plot(x, rep(0, n), type = "n", axes = FALSE, ylim = c(0, 1), xaxt = "n",
     xlab = "", ylab = "")
  lines(x, PseudoR2, col = "blue", lty = 1, lwd = 2)
  best.model <- 3
  axis(side=4, at = pretty(c(0, 1)))
  mtext("Pseudo R2", side=4, line=3)
  
  legend("topright",legend=c("BIC", "AIC","Pseudo R2"),
    text.col=c("red","red", "blue"), lty=c(1, 2, 1),col=c("red","red", "blue"))
  
})
```

### Plot & skit
Use the third model and plot the squared standardized Pearson residuals and the standardized deviance residuals against the linear predictor x^beta. Is there anything alarming here? Plot Cook’s distance against the linear predictor, as well as against higrads
and against region. Any interesting finds?

```{r}

xb <- predict(model.both)


cdi$v <- influence(model.both)$hat

#Pearson Standardized residuals
cdi$r <- influence(model.both)$pear.res / sqrt(1 - influence(model.both)$hat)
summary(cdi$r)

qqnorm(cdi$r)
qqline(cdi$r)
with(cdi, plot(r ~ xb))
abline(h = c(-2, 0, 2), col = "red", lty = 3)
#^2
with(cdi, plot(r^2 ~ xb))
abline(h = c(0, 2^2), col = "red", lty = 3)


#Squares standardized pearson residuals 
cdi$ds <- influence(model.both)$dev.res / sqrt(1 - influence(model.both)$hat)
summary(cdi$ds)
with(cdi, plot(ds ~ xb, 
                  ylab = "standardized deviance residuals"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)
#^2
with(cdi, plot(ds^2 ~ xb, 
                  ylab = "standardized deviance residuals"))
abline(h = c(-2, 0, 2), col = "red", lty = 3)



#Cook's
cdi$cook <- cooks.distance(model.both)
summary(cdi$cook)
with(cdi, plot(cook ~ xb, main = "Cook's distance"))
abline(h = c(1, 4 / n), col = "red", lty = 3)

with(cdi, plot(cook ~ region, main = "Cook's distance"))
abline(h = c(1, 4 / n), col = "red", lty = 3)

with(cdi, plot(cook ~ higrads, main = "Cook's distance"))
abline(h = c(1, 4 / n), col = "red", lty = 3)


```



### Interaction
One might suspect that the effect of higrads is different in different regions. Fit at fourth model adding the
interaction higrads * region to the third model, and use a Likelihood ratio test in order to determine
whether this is significantly better. Also calculate AIC, BIC, Nagelkerke, sensitivity and specificity and plot
the residuals and Cook’s distance. Do the interaction terms improve the model?

```{r}
model.interaction <- glm(hicrm ~ higrads*region, data = cdi, family = "binomial")

anova(model.both, model.interaction)

(Ddiff <- model.interaction$null.deviance - model.interaction$deviance)
# P-value for the test:
(dfdiff <- model.interaction$df.null - model.interaction$df.residual)


aicInteraction <- AIC(model.interaction)
bicInteraction <- AIC(model.interaction, k = log(nrow(cdi)))
sensitivityInteraction <- calc.metrics(model.interaction)[1]
specificityInteraction <- calc.metrics(model.interaction)[2]
R2pseudoInteraction <- pR2(model.interaction)[6]
 interaction.data <- cbind("aic" = aicInteraction, "bic" = bicInteraction, "sensitivity" = sensitivityInteraction, "spedificity" = specificityInteraction, "Pseudo R2" = R2pseudoInteraction)

rownames(interaction.data)[rownames(interaction.data)=="r2CU"] <- "Interaction model"

interaction.data


```



# Other variables
Find a better model using combinations of the variables higrads, region, poors and
phys1000 = 1000*phys/popul (see Lab 3). You may ignore interactions. Motivate why your model
is better.

```{R}
model.1 <- glm(hicrm ~ higrads, data = cdi, family = "binomial")
model.2 <- glm(hicrm ~ higrads + region, data = cdi, family = "binomial")
model.3 <- glm(hicrm ~ higrads + region + poors, data = cdi, family = "binomial")
model.4 <- glm(hicrm ~ higrads + region + poors + phys1000, data = cdi, family = "binomial")


model.names <- c("H", "H + R", "H + R + Poors", "H + R + Poors + Phys1000")
models <- list(model.1, model.2, model.3, model.4)



compare.models <- calc.metrics.models(models, model.names)

compare.models

 
```

```{r}

top.model <- glm(hicrm ~ region + poors + phys1000, data = cdi, family = "binomial")

metrics <- calc.metrics(top.model)
rownames(metrics) <- c("Best")

metrics

```


